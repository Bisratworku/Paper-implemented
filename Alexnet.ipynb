{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WvLVo7PzfrLw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=nb"
      ],
      "metadata": {
        "id": "MwxaNOGvB1BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Alexnet(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.Conv = nn.Sequential(\n",
        "        # 1st conv layer\n",
        "        nn.Conv2d(in_channels = 3 , out_channels = 96, kernel_size = 11, stride = 4, padding = 2),\n",
        "        nn.ReLU(),\n",
        "        nn.LocalResponseNorm(size = 5, alpha = 0.0001, beta = 0.75, k = 2),\n",
        "        nn.MaxPool2d(stride =2, kernel_size = 3),\n",
        "        # 2nd conv layer\n",
        "        nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size = 5, stride =1, padding = 2),\n",
        "        nn.ReLU(),\n",
        "        nn.LocalResponseNorm(size =5, alpha = 0.0001, beta = 0.75, k = 2 ),\n",
        "        nn.MaxPool2d(stride = 2, kernel_size = 3),\n",
        "        # 3rd conv layer\n",
        "        nn.Conv2d(in_channels = 256, out_channels = 384, stride = 1, padding = 1, kernel_size = 3),\n",
        "        nn.ReLU(),\n",
        "        #4th conv layer\n",
        "        nn.Conv2d(in_channels = 384, out_channels = 384, kernel_size = 3, padding = 1, stride = 1),\n",
        "        nn.ReLU(),\n",
        "        #5th conv layer\n",
        "        nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = 3, padding = 1, stride = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(stride = 2, kernel_size = 2)\n",
        "    )\n",
        "    self.Fc = nn.Sequential(\n",
        "        nn.Dropout(0,5),\n",
        "        nn.Linear(256 * 6 * 6, 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0,5),\n",
        "        nn.Linear(4096,4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4096, 10)\n",
        "    )\n",
        "    self.init_bias()\n",
        "  def init_bias(self):\n",
        "    for layer in self.Conv:\n",
        "      if isinstance(layer, nn.Conv2d):\n",
        "        nn.init.normal_(layer.weight, mean = 0, std = 0.01)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "    nn.init.constant_(self.Conv[4].bias, 1)\n",
        "    nn.init.constant_(self.Conv[10].bias, 1)\n",
        "    nn.init.constant_(self.Conv[12].bias, 1)\n",
        "  def forward(self, X):\n",
        "    X = self.Conv(X)\n",
        "    X = self.flatten(X)\n",
        "    X = self.Fc(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "0r1pNG9-YVBP"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}